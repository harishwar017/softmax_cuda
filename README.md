# CUDA Kernel Optimization for Softmax

### Naive â†’ Reduction-Based â†’ Warp-Level Optimized

This repository explores how **CUDA kernel** impacts performance for the **Softmax** function.

The goal is to progressively improve GPU efficiency through:

* version 1 : Naive implementations (baseline)
* version 2 : Reduction-based optimizations
* version 3 : Warp-level and fused-kernel designs

---

## Implementations

| Version               | Description                                                                     |
| :-------------------- | :------------------------------------------------------------------------------ |
| **Naive**             | Direct global memory access, atomic operations, multiple kernel launches.       |
| **Reduction-Based**   | Uses shared-memory reductions to reduce contention and improve throughput.      |
| **(Next) Warp-Level** | *Planned:* Fuse kernels and use warp shuffle intrinsics for maximum efficiency. |

---

## âš–ï¸ Comparison Overview

| Implementation      | Passes | Memory Access |  Efficiency | Notes                               |
| :------------------ | :----: | :-----------: | :---------: | :---------------------------------- |
| **Naive**           |    3   |      High     |    ğŸ”´ Low   | Atomic ops & race conditions        |
| **Reduction-Based** |    2   |     Medium    | ğŸŸ  Moderate | Shared memory, but multi-pass       |
| **Fused (Online)**  |    1   |      Low      |   ğŸŸ¢ High   | FlashAttention-inspired single-pass |

---

## Comparison Summary

| Kernel            |                   Naive                  |                Reduction-Based               |         Next (Planned)        |
| :---------------- | :--------------------------------------: | :------------------------------------------: | :---------------------------: |
| **Addition**      | Very low efficiency (atomic bottlenecks) | âœ… Major speedup with shared-memory reduction |    ğŸ”œ Warp-level reduction    |
| **Normalization** |             Already efficient            |           âœ… Consistent performance           |   ğŸ”œ Fine-tune memory access  |
| **Softmax**       |         Multi-pass & memory-bound        |             âš™ï¸ Slight improvement            | ğŸ”œ Fuse exp + sum + normalize |

---

## Key Takeaways

* **Atomic operations** severely limit performance â€” reduction-based design solves this.
* **Shared memory** drastically improves efficiency by reducing global memory traffic.
* **Kernel fusion** and **warp-level primitives** are the next logical steps for further speedups.

---

## Next Version (In Progress)

* Implement **warp-level reductions** (`__shfl_xor_sync`, etc.)
* **Fuse softmax passes** (exponentiation + reduction + normalization)
* Optimize **memory coalescing** and **occupancy**

---

## Structure

```
cuda-kernels/
â”œâ”€â”€ softmax_v1(naive)/
â”œâ”€â”€ softmax_v2(reduction based)/
â”œâ”€â”€ softmax_v3(warp level optimised)/
â””â”€â”€ README.md
```