# CUDA Kernel Optimization for Softmax

### Naive ‚Üí Reduction-Based ‚Üí Warp-Level Optimized

This repository explores how **CUDA kernel** impacts performance for the **Softmax** function.

The goal is to progressively improve GPU efficiency through:

version 1 : Naive implementations (baseline)
version 2 : Reduction-based optimizations
version 3 : Warp-level and fused-kernel designs

---

## Implementations

| Version               | Description                                                                     |
| :-------------------- | :------------------------------------------------------------------------------ |
| **Naive**             | Direct global memory access, atomic operations, multiple kernel launches.       |
| **Reduction-Based**   | Uses shared-memory reductions to reduce contention and improve throughput.      |
| **(Next) Warp-Level** | *Planned:* Fuse kernels and use warp shuffle intrinsics for maximum efficiency. |

---

## Comparison Summary

| Kernel            |                   Naive                  |                Reduction-Based               |         Next (Planned)        |
| :---------------- | :--------------------------------------: | :------------------------------------------: | :---------------------------: |
| **Addition**      | Very low efficiency (atomic bottlenecks) | ‚úÖ Major speedup with shared-memory reduction |    üîú Warp-level reduction    |
| **Normalization** |             Already efficient            |           ‚úÖ Consistent performance           |   üîú Fine-tune memory access  |
| **Softmax**       |         Multi-pass & memory-bound        |             ‚öôÔ∏è Slight improvement            | üîú Fuse exp + sum + normalize |

---

## Key Takeaways

* **Atomic operations** severely limit performance ‚Äî reduction-based design solves this.
* **Shared memory** drastically improves efficiency by reducing global memory traffic.
* **Kernel fusion** and **warp-level primitives** are the next logical steps for further speedups.

---

## Next Version (In Progress)

* Implement **warp-level reductions** (`__shfl_xor_sync`, etc.)
* **Fuse softmax passes** (exponentiation + reduction + normalization)
* Optimize **memory coalescing** and **occupancy**

---

## Structure

```
cuda-kernels/
‚îú‚îÄ‚îÄ softmax_v1(naive)/
‚îú‚îÄ‚îÄ softmax_v2(reduction based)/
‚îú‚îÄ‚îÄ softmax_v3(warp level optimised)/
‚îî‚îÄ‚îÄ README.md
```